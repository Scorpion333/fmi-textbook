# Представяне на величините в паметта

То може да бъде различно, в зависимост от операционната система и компилатора.
Описаната по-долу схема може и да не е съвсем вярна за всеки компютър, но със
сигурност е подобна и разминаванията са най-вече в цифрите, напр. при мен int заема 4B, но при вас може да е 8В.

## Цели числа

Цяло число се записва чрез двоичното си представяне, напр. 790 = 11 0001 0110.
Ако int заема 4В, числото ще се представи с 32 бита.
Те обаче не са подредени "правилно", т.е. 790 няма да се запише като 0000-0000 0000-0000 0000-0011 0001-0110, а ето така:

    Байт X     Байт Х+1   Байт X+2   Байт X+3
    0001-0110  0000-0011  0000-0000  0000-0000
    
Изглежда малко странно, защото най-предният байт съдържа последните 8 двоични цифри, а последният байт съдържа първите,
но за това подреждане има причина - то улеснява кастването. Хубавото е, че компилаторите не се объркват, за разлика от програмистите. ;)

## По-големи и по-малки числа

Ако типът е long или long long, се случва същото - само бройт на байтовете е по-голям.

Положителните числа се записват по еднакъв начин, независимо дали типът им е unsigned или int.
За отрицателните int-ове се използва правилото 2's complement, което няма да разглеждаме.

## Bool и Char

И двата типа заемат 1 байт - най-малкия възможен размер за променлива (или константа).

True се записва като 0000 0001, а false като 0000 0000, т.е. за bool се използва двоичните записи на числата 1 и 0.
Въпреки че е само последният бит е съществен, се налага да изгубим другите 7, заделяйки цял байт.

Един символ (char) се записва чрез двоичното представяне на ASCII кода си, например 'J' (ASCII: 74) = 0100 1010.

Възможните стойности на char обаче са 256, което е повече от символите в ASCII таблицата (128), затова всеки байт е валиден char.

Това позволява произволна последователност от байтове да бъде обработвана като масив от char-ове, което се използва при двоичните файлове.